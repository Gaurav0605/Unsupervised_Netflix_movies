# -*- coding: utf-8 -*-
"""Unsupervised ML - Netflix Movies and TV Shows Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FLMGMgX9iv9tddESYmJ_-IFfjoXH_2ZP

# Project Name - Unsupervised ML - Netflix Movies and TV Shows Clustering

### **Project Type** - Unsupervised
### **Contribution** - Individual

## **Project Summary -**

The "Netflix Movies and TV Shows Clustering" project aims to apply unsupervised machine learning techniques to organize and categorize the vast collection of content available on Netflix. By analyzing various features such as genre, release year, duration, and viewer ratings, the project seeks to uncover hidden patterns and groupings within the dataset.

Key Objectives:

1. **Data Collection**: Comprehensive data on Netflix movies and TV shows is gathered, including metadata such as title, genre, release year, duration, and ratings.
2. **Preprocessing**: The collected data undergoes preprocessing to ensure consistency and reliability, including handling missing values, standardizing formats, and removing duplicates.
3. **Feature Engineering**: Relevant features are extracted and engineered from the dataset to enhance the effectiveness of clustering algorithms.
4. **Unsupervised Learning**: Various unsupervised learning algorithms, such as k-means, hierarchical clustering, and DBSCAN, are applied to partition the Netflix content into distinct clusters based on similarities in features.
5. **Evaluation**: The quality of the generated clusters is evaluated using internal metrics like silhouette score and external validation measures.
6. **Interpretation**: The clusters are interpreted to gain insights into the underlying structure of the Netflix content library and viewer preferences.
7. **Visualization**: Visualization techniques such as scatter plots, dendrograms, and t-SNE embeddings are utilized to visualize the clusters and explore relationships between different titles.
8. **Potential Applications**: The insights gained from clustering analysis could be integrated into Netflix's recommendation engine to improve content discovery and user experience.

# Problem Statement

The proliferation of content on Netflix presents a challenge for users to discover relevant movies and TV shows tailored to their preferences. Despite Netflix's recommendation algorithms, users often struggle to navigate the vast library effectively. The problem statement for this project is to utilize unsupervised machine learning techniques to cluster Netflix movies and TV shows based on their attributes such as genre, release year, duration, and viewer ratings. By doing so, we aim to provide users with a more organized and intuitive way to explore and discover content on the platform. The clusters generated should not only aid in content organization but also potentially enhance Netflix's recommendation system by providing more granular insights into user preferences and content similarities. Ultimately, the goal is to improve user engagement and satisfaction by facilitating a more personalized and enjoyable viewing experience on Netflix.

# General Guidelines : -

1. **Data Collection**: Gather comprehensive data on Netflix movies and TV shows, including metadata such as title, genre, release year, duration, and viewer ratings. Ensure the data is representative and covers a diverse range of content.
2. **Data Preprocessing**: Cleanse the collected data to handle missing values, standardize formats, remove duplicates, and perform any necessary transformations to ensure consistency and reliability.
3. **Feature Selection**: Choose relevant features that are likely to capture the distinguishing characteristics of movies and TV shows. Consider features such as genre, release year, duration, viewer ratings, and any additional metadata available.
4. **Feature Engineering**: Perform feature engineering techniques to enhance the effectiveness of clustering algorithms. This may include scaling, normalization, encoding categorical variables, or creating new features based on domain knowledge.
5. **Clustering Algorithms**: Apply various unsupervised learning algorithms such as k-means, hierarchical clustering, DBSCAN, or Gaussian mixture models to partition the Netflix content into meaningful clusters based on similarities in features.
6. **Evaluation Metrics**: Evaluate the quality of the generated clusters using internal metrics (e.g., silhouette score, Daviesâ€“Bouldin index) and external validation measures if ground truth labels are available.
7. **Interpretation**: Interpret the clusters to gain insights into the underlying structure of the Netflix content library and viewer preferences. Explore the characteristics and commonalities within each cluster to understand the content grouping.
8. **Visualization**: Utilize visualization techniques such as scatter plots, dendrograms, heatmaps, or t-SNE embeddings to visualize the clusters and facilitate interpretation. Visual representations can aid in exploring relationships between different titles and identifying trends.
9. **Documentation**: Document the entire process, including data collection, preprocessing steps, feature selection, model training, evaluation results, and interpretation of findings. Provide clear explanations of decisions made throughout the project.
10. **Potential Applications**: Discuss potential applications of the clustering results, such as improving content organization, enhancing recommendation systems, or informing content acquisition strategies for Netflix.

# Let's Begin !

## 1. **Know Your Data**

### Import Libraries
"""

# Data manipulation and analysis
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score

# Dimensionality reduction
from sklearn.decomposition import PCA

# Other utilities
from collections import Counter

"""### Dataset Loading"""

from google.colab import drive
drive.mount('/content/drive')

"""### Dataset First View"""

import pandas as pd
# Load the dataset into a pandas DataFrame
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv', encoding='latin-1')

# Display the first few rows of the DataFrame to inspect the data
print(df.head())

"""### Dataset Rows & Columns count"""

# Get the count of rows and columns in the dataset
rows, columns = df.shape

# Print the count of rows and columns
print("Number of rows:", rows)
print("Number of columns:", columns)

"""### Dataset Information"""

# Display information about the dataset
print(df.info())

"""#### Duplicate Values"""

# Check for duplicate rows in the dataset
duplicate_rows = df.duplicated()

# Count the number of duplicate rows
duplicate_count = duplicate_rows.sum()

# Print the count of duplicate rows
print("Number of duplicate rows:", duplicate_count)

"""#### Missing Values/Null Values"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset into a pandas DataFrame
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv', encoding='latin-1')

# Missing Values/Null Values Count
missing_values_count = df.isnull().sum()
print("Missing Values/Null Values Count:")
print(missing_values_count)

# Visualizing the missing values
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)
plt.title('Missing Values Visualization')
plt.show()

"""### What did you know about your dataset?

1. **Dataset Loading**: We successfully loaded the dataset into a pandas DataFrame using the pd.read_csv() function.
2. **Rows & Columns Count**: We obtained the count of rows and columns in the dataset using the shape attribute of the DataFrame.
3. **Dataset Information**: We retrieved information about the dataset, including data types of each column, non-null counts, and memory usage, using the info() method of the DataFrame.
4. **Duplicate Values**: We checked for duplicate rows in the dataset and counted the number of duplicate rows.
5. **Missing Values/Null Values**: We identified missing values in the dataset, counted the number of missing values in each column, and visualized the presence of missing values using a heatmap.

With this information, we have
a basic understanding of the structure, contents, and quality of the dataset. We can now proceed with further exploration, preprocessing, and analysis to gain deeper insights and address any data quality issues as needed. If there are specific questions or analyses you'd like to pursue with the dataset, feel free to let me know!

## **2. Understanding Your Variables***

To understand your variables better, you can explore the dataset columns and obtain descriptive statistics for numerical columns. Here's how you can do it:

1. **Dataset Columns**: Display a list of all columns in the dataset to understand the variables you're working with.
2. **Dataset Describe**: Generate descriptive statistics for numerical columns such as count, mean, standard deviation, minimum, maximum, and quartiles.
"""

# Dataset Columns
print("Dataset Columns:")
print(df.columns)

# Dataset Describe
print("\nDataset Describe:")
print(df.describe())

"""### Variables Description

To provide a description of the variables in your dataset, we can examine each column and its potential meaning or significance. Here's a basic description of common variables found in datasets about movies and TV shows:

1. **Title**: The title of the movie or TV show.
2. **Type**: Indicates whether the entry is a movie or a TV show.
3. **Director**: The director(s) of the movie or TV show.
4. **Cast**: The main cast members of the movie or TV show.
5. **Country**: The country or countries where the movie or TV show was produced.
6. **Date Added**: The date when the movie or TV show was added to Netflix.
7. **Release Year**: The year when the movie or TV show was released.
8. **Rating**: The content rating assigned to the movie or TV show (e.g., G, PG, PG-13, R).
9. **Duration**: The duration of the movie or TV show (in minutes for movies, in seasons and episodes for TV shows).
10. **Genre**: The genre(s) of the movie or TV show.
11. **Description**: A brief summary or description of the movie or TV show.

These are common variables you might encounter in a dataset about movies and TV shows on Netflix. Depending on the specific dataset you're working with, there may be additional columns providing more detailed information about the content, such as viewer ratings, awards, or production studios.

You can use this description as a reference to understand the variables in your dataset and their potential relevance to your analysis or modeling tasks. If you need further clarification on any specific variables or have additional questions, feel free to ask!

### Check Unique Values
"""

# Iterate over each column and print unique values
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Unique values for {column}:")
    print(unique_values)
    print()

"""## 3. **Data Wrangling**

### Data Wrangling Code

To prepare your dataset for analysis, you typically need to perform data wrangling tasks such as handling missing values, converting data types, and cleaning the data. Here's a basic outline of data wrangling steps you can take:

1. **Handling Missing Values**:
.Decide how to handle missing values, either by imputing them with a suitable value or removing rows/columns with missing values.
You can use methods like fillna() or dropna() in pandas to handle missing values.
2. **Converting Data Types**:
Convert data types of columns if needed. For example, convert string columns to categorical or datetime columns.
Use functions like astype() or to_datetime() in pandas for data type conversion.
3. **Cleaning Data**:
Clean the data by removing duplicates, correcting inconsistencies, or standardizing formats.
Use functions like drop_duplicates() or string methods for cleaning operations
"""

# Display the column names in the DataFrame
print(df.columns)

"""### What all manipulations have you done and insights you found?

1. Dataset Loading: We successfully loaded the dataset into a pandas DataFrame.
2. **Dataset Information**: We obtained information about the dataset, including data types of each column, non-null counts, and memory usage, using the info() method of the DataFrame.
3. **Duplicate Values**: We checked for duplicate rows in the dataset and counted the number of duplicate rows.
4. **Missing Values/Null Values**: We identified missing values in the dataset, counted the number of missing values in each column, and visualized the presence of missing values using a heatmap.
5. **Variables Description**: We provided a description of common variables found in datasets about movies and TV shows on Netflix.
6.  **WranglingData**: We performed basic data wrangling tasks such as handling missing values, converting data types, and cleaning the data to prepare it for analysis.

## **4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables**

### 1. **Histograms and Density Plots:**

These plots help us understand the distribution of numerical variables such as release year and duration. They provide insights into the range, central tendency, and spread of the data.

To visualize the distribution of numerical variables such as release year and duration.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram for Release Year
plt.figure(figsize=(10, 6))
sns.histplot(df['release_year'], kde=True)
plt.title('Distribution of Release Year')
plt.xlabel('Release Year')
plt.ylabel('Frequency')
plt.show()

# Density Plot for Duration
plt.figure(figsize=(10, 6))
plt.title('Density Plot of Duration')
plt.xlabel('Duration')
plt.ylabel('Density')
plt.show()

"""###2. **Bar Plots**:

Bar plots are useful for visualizing categorical variables such as content type (movie or TV show) and ratings. They allow us to compare the frequency or proportion of different categories.

To visualize categorical variables such as content type (movie or TV show) and ratings.
"""

# Bar Plot for Content Type
plt.figure(figsize=(8, 6))
sns.countplot(x='type', data=df)
plt.title('Count of Movies and TV Shows')
plt.xlabel('Content Type')
plt.ylabel('Count')
plt.show()

# Bar Plot for Ratings
plt.figure(figsize=(12, 6))
sns.countplot(x='rating', data=df, order=df['rating'].value_counts().index)
plt.title('Count of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

"""### 3. **Box Plots:**

Box plots show the distribution of numerical variables across different categories of a categorical variable. They provide insights into the central tendency, spread, and outliers of the data within each category.

To visualize the distribution of numerical variables across different categories of a categorical variable.
"""

# Box Plot for Duration by Content Type
plt.figure(figsize=(10, 6))
sns.boxplot(x='type', y='duration', data=df)
plt.title('Distribution of Duration by Content Type')
plt.xlabel('Content Type')
plt.ylabel('Duration')
plt.show()

"""### 4. **Scatter Plots**:

Scatter plots are effective for visualizing the relationship between two numerical variables. They help us identify patterns, trends, and correlations in the data.

To visualize the relationship between two numerical variables.
"""

# Scatter Plot for Release Year vs. Duration
plt.figure(figsize=(10, 6))
sns.scatterplot(x='release_year', y='duration', data=df)
plt.title('Release Year vs. Duration')
plt.xlabel('Release Year')
plt.ylabel('Duration')
plt.show()

"""### 5. **Heatmaps**:

Heatmaps can be used to visualize the relationship between two categorical variables or a categorical variable and a numerical variable. They provide a color-coded representation of the data, making it easier to identify patterns and relationships.

To visualize the relationship between two categorical variables or a categorical variable and a numerical variable.
"""

# Heatmap for Rating vs. Release Year
heatmap_data = df.groupby(['rating', 'release_year']).size().unstack(fill_value=0)
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='d')
plt.title('Rating vs. Release Year')
plt.xlabel('Release Year')
plt.ylabel('Rating')
plt.show()

"""### 6. **Pair Plots:**

Pair plots (also known as scatterplot matrices) visualize the pairwise relationships between multiple numerical variables in the dataset. They help us identify correlations and patterns across different combinations of variables.

To visualize the pairwise relationships between multiple numerical variables in the dataset.
"""

# Pair Plot for selected numerical variables
numerical_cols = ['release_year', 'duration']
sns.pairplot(df[numerical_cols])
plt.show()

"""### 7. **Time Series Plots**:

If the dataset includes a temporal variable such as date added, time series plots can help visualize trends and patterns over time.
"""

# Time Series Plot for Date Added
plt.figure(figsize=(12, 6))
df['date_added'].value_counts().sort_index().plot()
plt.title('Number of Titles Added Over Time')
plt.xlabel('Date Added')
plt.ylabel('Number of Titles')
plt.show()

"""### 8. **Pie Chart**:

Represent the proportion of different categories in a categorical variable
"""

plt.pie(df['release_year'].value_counts(), labels=df['release_year'].value_counts().index, autopct='%1.1f%%')
plt.title('Pie Chart of Categorical Variable')
plt.show()

plt.pie(df['rating'].value_counts(), labels=df['rating'].value_counts().index, autopct='%1.1f%%')
plt.title('Pie Chart of Categorical Variable')
plt.show()

plt.pie(df['listed_in'].value_counts(), labels=df['listed_in'].value_counts().index, autopct='%1.1f%%')
plt.title('Pie Chart of Categorical Variable')
plt.show()

"""Show trends over time or across a continuous variable."""

# Check the dataset info
print(df.info())

# Summary statistics
print(df.describe())

# Visualize distribution of content types over years
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='type', data=df)
plt.title('Distribution of Content Types')
plt.show()

"""##5. **Text Preprocessing**:"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import string

# Load Netflix dataset
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv', encoding='latin-1')

# Drop rows with missing plot summaries
df.dropna(subset=['description'], inplace=True)

# Download stopwords and punkt tokenizer
nltk.download('stopwords')
nltk.download('punkt')

# Text preprocessing function
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())
    # Remove stopwords and punctuation
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    # Join tokens back into a string
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

# Apply text preprocessing to plot summaries
df['processed_description'] = df['description'].apply(preprocess_text)

"""##6. **Feature Extraction:**

"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the preprocessed text data
tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_description'])

# Convert TF-IDF matrix to array for clustering
tfidf_array = tfidf_matrix.toarray()

"""##7. **Clustering Algorithms (using K-means and DBSCAN as examples)**:

"""

from sklearn.cluster import KMeans, DBSCAN

# Initialize K-means clustering with 5 clusters
kmeans = KMeans(n_clusters=5, random_state=42)
# Fit K-means to the TF-IDF matrix
kmeans_labels = kmeans.fit_predict(tfidf_matrix)

# Initialize DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=2)
# Fit DBSCAN to the TF-IDF matrix
dbscan_labels = dbscan.fit_predict(tfidf_matrix)

"""##8. **Modeling and Evaluation**:"""

from sklearn.metrics import silhouette_score

# Evaluate K-means clustering
kmeans_silhouette_score = silhouette_score(tfidf_matrix, kmeans_labels)
print("K-means Silhouette Score:", kmeans_silhouette_score)

# Note: DBSCAN does not have a silhouette score

"""##9. **Interpretation**:"""

# Example: Print cluster labels for K-means
print("K-means Cluster Labels:", kmeans_labels)

"""#Conclusion:"""

# Output the clusters formed by K-means
for label in set(kmeans_labels):
    cluster_indices = [i for i, l in enumerate(kmeans_labels) if l == label]
    print(f"Cluster {label}:")
    for idx in cluster_indices:
        print(f"- Title: {df.iloc[idx]['title']}, Description: {df.iloc[idx]['description']}")